export interface DsaTopic {
  id: string;
  name: string;
  difficulty: "Fundamental" | "Intermediate" | "Advanced" | "Expert";
  timeEstimate: string;
  domains: string[];
  description: string;
  concepts: string[];
  keyPoints: string[];
  timeComplexity?: string;
  spaceComplexity?: string;
  commonPatterns: string[];
  practiceProblems: number[];
}

export const dsaTopics: DsaTopic[] = [
  {
    id: "arrays-strings",
    name: "Arrays & Strings",
    difficulty: "Fundamental",
    timeEstimate: "2-3 weeks",
    domains: ["Array", "String"],
    description:
      "# Arrays & Strings\n\nArrays and strings are the foundational building blocks of data structures in computer science. They allow you to store collections of elements (arrays) or sequences of characters (strings) in contiguous memory, enabling efficient access and manipulation.\n\n## 1. Overview\n\n- **Array**: A fixed‑size, indexed collection of elements of the same type.\n- **String**: A sequence of characters, often implemented as an array of code units (e.g., ASCII or Unicode).\n\n## 2. Core Operations\n\n1. **Traversal & Indexing**  \n   - Access any element in _O(1)_ time via its index.<br>\n   - Iterate in _O(n)_ time for n elements.\n2. **Insertion & Deletion**  \n   - In arrays, insertion/deletion at arbitrary positions is _O(n)_ (elements must be shifted).<br>\n   - At the end (append/remove last), arrays can be _amortized O(1)_ in dynamic implementations.\n3. **Search**  \n   - Linear search: _O(n)_.<br>\n   - Binary search (sorted arrays): _O(log n)_ (requires random access).\n\n## 3. Common Techniques\n\n- **Two‑Pointer Technique**  \n  Use two indices moving towards each other (or in tandem) for problems like pair sums, palindrome checks, and partitioning.\n\n- **Sliding Window**  \n  Maintain a window range `[i..j]` and adjust boundaries to find subarrays/substrings that satisfy constraints (e.g., maximum sum, unique characters).\n\n- **Fast & Slow Pointers**  \n  Advance one pointer twice as fast as the other to detect cycles or find midpoints.\n\n## 4. String‑Specific Tips\n\n- **Immutability** (in languages like Java, Python): Construct new strings for modifications—watch out for _O(n)_ overhead.\n- **Character Encoding**: Understand UTF‑8 vs. UTF‑16 when indexing by code units vs. code points.\n- **Common Operations**: substring extraction, concatenation, search (indexOf/KMP), replace, split/join.\n\n## 5. Edge Cases & Best Practices\n\n- **Empty Arrays/Strings**: Always check length before accessing indices.\n- **Single‑Element Cases**: Ensure loops handle n=1 correctly.\n- **Overflow**: Be cautious when computing `i + j` as a midpoint in some languages—use `i + (j - i) / 2` to avoid overflow.\n\n## 6. Resources & Further Reading\n\n- [GeeksforGeeks: Array Data Structure](https://www.geeksforgeeks.org/array-data-structure/)  \n- [LeetCode Explore: Arrays & Strings](https://leetcode.com/explore/learn/card/array-and-string/)  \n- [CLRS §10: Elementary Data Structures](https://mitpress.mit.edu/books/introduction-algorithms-third-edition)\n",
    concepts: [
      "Array indexing and traversal",
      "Two-pointer technique",
      "Sliding window pattern",
      "String manipulation",
      "Character encoding and ASCII",
      "In-place algorithms",
    ],
    keyPoints: [
      "Arrays provide O(1) random access to elements",
      "Strings are immutable in many languages",
      "Two-pointer technique helps reduce time complexity",
      "Sliding window is useful for substring problems",
      "Consider edge cases like empty arrays/strings",
    ],
    timeComplexity:
      "Access: O(1), Search: O(n), Insertion: O(n), Deletion: O(n)",
    spaceComplexity: "O(1) for iteration, O(n) for additional space",
    commonPatterns: [
      "Two Sum pattern",
      "Sliding Window",
      "Fast & Slow pointers",
      "Merge intervals",
      "Cyclic sort",
    ],
    practiceProblems: [1, 121, 53, 15, 42, 20, 5, 76],
  },

  {
    id: "linked-lists",
    name: "Linked Lists",
    difficulty: "Fundamental",
    timeEstimate: "1-2 weeks",
    domains: ["Linked List"],
    description:
      "# Linked Lists\n\nLinked lists are **dynamic**, linear data structures composed of nodes, where each node stores data and a reference (pointer) to the next (and/or previous) node. Unlike arrays, they allow fast insertions and deletions without shifting elements.\n\n## 1. Types of Linked Lists\n\n- **Singly Linked List**  \n  Each node has a `next` pointer. Traversal is one–way from head to tail.\n\n- **Doubly Linked List**  \n  Nodes have both `next` and `prev` pointers, allowing forward and backward traversal.\n\n- **Circular Linked List**  \n  The last node’s `next` pointer links back to the head, forming a circle (applicable to both singly and doubly lists).\n\n## 2. Core Operations\n\n1. **Insertion**  \n   - **At Head**: Create new node, point its `next` to current head, update head pointer → _O(1)_.\n   - **At Tail**: Traverse to end, link new node → _O(n)_ if no tail pointer; _O(1)_ with tail reference.\n   - **At Position**: Traverse to position, adjust pointers → _O(n)_.\n\n2. **Deletion**  \n   - **At Head**: Move head to `head.next`, free old head → _O(1)_.\n   - **At Tail**: Traverse to second-last, set its `next` to `null` → _O(n)_.\n   - **By Value/Position**: Locate node, update pointers of neighbors → _O(n)_.\n\n3. **Traversal & Search**  \n   - Walk nodes via `next` (and `prev` if doubly) → _O(n)_ to visit all or find a value.\n\n4. **Access**  \n   - No direct random access—must traverse from head for index-based access → _O(n)_.\n\n## 3. Common Patterns & Techniques\n\n- **Fast & Slow Pointers**  \n  Detect cycles or find the midpoint by moving one pointer twice as fast as the other.\n\n- **List Reversal**  \n  Iteratively or recursively reverse `next` pointers to invert the list → _O(n)_ time, _O(1)_ space (iterative).\n\n- **Merge Two Sorted Lists**  \n  Walk both lists simultaneously, splicing nodes in sorted order.\n\n- **Cycle Detection (Floyd’s Algorithm)**  \n  Fast & slow pointers meet if a cycle exists; find entry point by resetting one pointer to head.\n\n- **Split & Reassemble**  \n  Divide list at midpoint for divide‑and‑conquer (e.g., merge sort on lists).\n\n## 4. Memory & Edge Cases\n\n- **Null References**: Always check for `null` before accessing `next`/`prev` pointers.\n- **Empty List**: Head pointer is `null`; handle insert/delete accordingly.\n- **Single-Node List**: Both head and tail (if maintained) point to the same node.\n- **Memory Overhead**: Each node stores extra pointer(s) → increased space compared to arrays.\n\n## 5. When to Use\n\n- Frequent insertions/deletions at arbitrary positions.\n- When data size is unknown or varies often.\n- Implementing stacks, queues, hash table chaining, adjacency lists for graphs.\n\n## 6. Resources & Further Reading\n\n- [GeeksforGeeks: Linked List Data Structure](https://www.geeksforgeeks.org/data-structures/linked-list/)  \n- [LeetCode Explore: Linked Lists](https://leetcode.com/explore/learn/card/linked-list/)  \n- CLRS §10.1: Linked Lists (Introduction to Algorithms)\n",
    concepts: [
      "Singly linked lists",
      "Doubly linked lists",
      "Circular linked lists",
      "Node manipulation",
      "Pointer arithmetic",
      "List traversal patterns",
    ],
    keyPoints: [
      "Dynamic size allocation",
      "Efficient insertion/deletion at beginning",
      "No random access to elements",
      "Extra memory overhead for pointers",
      "Be careful with null pointer exceptions",
    ],
    timeComplexity:
      "Access: O(n), Search: O(n), Insertion: O(1), Deletion: O(1)",
    spaceComplexity: "O(1) for operations, O(n) for storage",
    commonPatterns: [
      "Fast & Slow pointers",
      "Reverse a linked list",
      "Merge sorted lists",
      "Cycle detection",
      "Find middle element",
    ],
    practiceProblems: [206, 21, 23],
  },
  {
    id: "stacks-queues",
    name: "Stacks & Queues",
    difficulty: "Fundamental",
    timeEstimate: "1-2 weeks",
    domains: ["Stack", "Queue"],
    description:
      "# Stacks & Queues\n\nStacks and queues are fundamental abstract data types that organize elements according to specific ordering rules:\n\n- **Stack**: Last In, First Out (LIFO)\n- **Queue**: First In, First Out (FIFO)\n\n## 1. Core Operations\n\n### 1.1 Stack Operations\n- `push(x)`: Add element `x` to the top of the stack.\n- `pop()`: Remove and return the top element. Underflow if empty.\n- `peek()` / `top()`: Return the top element without removing it.\n- `isEmpty()`: Check if the stack is empty.\n\n### 1.2 Queue Operations\n- `enqueue(x)`: Add element `x` to the back of the queue.\n- `dequeue()`: Remove and return the front element. Underflow if empty.\n- `front()` / `peek()`: Return the front element without removing it.\n- `isEmpty()`: Check if the queue is empty.\n\n## 2. Variants & Advanced Types\n\n- **Circular Queue**: Fixes wasted space by wrapping the rear pointer to the front when the end of the buffer is reached.\n- **Deque (Double‑Ended Queue)**: Supports insertion and deletion at both ends in _O(1)_.\n- **Priority Queue**: Elements are dequeued in priority order (often implemented with a heap).\n- **Monotonic Stack**: Maintains elements in increasing or decreasing order to solve problems like next‑greater element.\n\n## 3. Use Cases & Patterns\n\n- **Balanced Parentheses**: Use a stack to match open/close symbols.\n- **Function Call Stack**: Language runtimes use a stack to track active function calls.\n- **Expression Evaluation**: Convert infix to postfix (Shunting‑Yard) and evaluate with a stack.\n- **Undo/Redo**: Maintain two stacks to support undo and redo operations.\n- **Breadth‑First Search (BFS)**: Use a queue to traverse graph layers in FIFO order.\n- **Level Order Traversal**: Similar to BFS on tree structures.\n\n## 4. Implementation Details & Edge Cases\n\n- **Overflow/Underflow**: Always check capacity or emptiness before `push`/`enqueue` and `pop`/`dequeue`.\n- **Dynamic Resizing**: Use a dynamic array or linked list to implement stacks/queues that grow as needed.\n- **Pointer/Wrapping Logic**: In circular queues, carefully manage `head` and `tail` indices modulo buffer size.\n- **Thread Safety**: In concurrent environments, use locks or lock‑free algorithms.\n\n## 5. Time & Space Complexity\n\n- **Push/Pop/Enqueue/Dequeue**: _O(1)_\n- **Space**: _O(n)_ for storing n elements (plus _O(1)_ overhead per node or index).\n\n## 6. Best Practices\n\n- Choose an implementation (array vs. linked list) based on your language and performance needs.\n- For fixed‑capacity stacks/queues, preallocate to avoid frequent resizing.\n- Document and handle error conditions (underflow/overflow) explicitly.\n\n## 7. Further Reading & Resources\n\n- [GeeksforGeeks: Stack Data Structure](https://www.geeksforgeeks.org/stack-data-structure/)\n- [GeeksforGeeks: Queue Data Structure](https://www.geeksforgeeks.org/queue-data-structure/)\n- [LeetCode Explore: Stacks & Queues](https://leetcode.com/explore/learn/card/queue-stack/)\n",
    concepts: [
      "LIFO and FIFO principles",
      "Stack operations (push, pop, peek)",
      "Queue operations (enqueue, dequeue)",
      "Circular queues",
      "Priority queues",
      "Monotonic stacks",
    ],
    keyPoints: [
      "Stacks are useful for nested structures",
      "Queues are essential for BFS algorithms",
      "Consider using deque for both ends access",
      "Stack overflow and underflow conditions",
      "Applications in parsing and evaluation",
    ],
    timeComplexity: "Push/Pop/Enqueue/Dequeue: O(1)",
    spaceComplexity: "O(n) for storage",
    commonPatterns: [
      "Balanced parentheses",
      "Expression evaluation",
      "Function call stack",
      "Undo operations",
      "Level order traversal",
    ],
    practiceProblems: [20, 155],
  },
  {
    id: "trees-bst",
    name: "Trees & Binary Search Trees",
    difficulty: "Intermediate",
    timeEstimate: "3-4 weeks",
    domains: ["Tree", "Binary Search"],
    description:
      "# Trees & Binary Search Trees\n\nTrees are **hierarchical** data structures composed of nodes connected by edges, with a single **root** and zero or more **child** nodes. A **Binary Search Tree (BST)** is a special binary tree that maintains a sorted order: for any node, all values in its left subtree are less, and all in its right subtree are greater.\n\n## 1. Terminology & Properties\n\n- **Node**: Contains a value and pointers to child nodes.\n- **Root**: Topmost node with no parent.\n- **Leaf**: Node with no children.\n- **Height**: Length of the longest path from node to leaf.\n- **Depth**: Distance from root to the node.\n- **Subtree**: Any node and all its descendants.\n\n## 2. Tree Types\n\n- **Binary Tree**: Each node has at most two children (`left`, `right`).\n- **N‑ary Tree**: Nodes can have _n_ children.\n- **Balanced Tree**: Height is kept to _O(log n)_ using rotations or rebalancing (e.g., AVL, Red‑Black).\n\n## 3. Traversal Techniques\n\n1. **Preorder** (Root, Left, Right)  \n2. **Inorder** (Left, Root, Right) — yields sorted order in BSTs.  \n3. **Postorder** (Left, Right, Root)  \n4. **Level‑Order (BFS)** using a queue.\n\n## 4. BST Core Operations\n\n- **Search**: Compare with current node, recurse left or right → _O(h)_.\n- **Insert**: Find leaf position and attach new node → _O(h)_.\n- **Delete**:\n  1. **Leaf**: Remove directly.\n  2. **One Child**: Replace node with its child.\n  3. **Two Children**: Swap with inorder successor (or predecessor), then delete.\n\n> _h_ = tree height: _O(log n)_ if balanced, _O(n)_ worst case.\n\n## 5. Balancing & Self‑Balancing Trees\n\n- **Why?** Unbalanced insertion can degrade BST to a linked list.\n- **AVL Trees**: Maintain |height(left) − height(right)| ≤ 1 with rotations.\n- **Red‑Black Trees**: Enforce color and black‑height properties for balance.\n\n## 6. Common Patterns & Problems\n\n- **Lowest Common Ancestor**: Find split point of two nodes in BST or binary tree.\n- **Path Sum**: Check if root‑to‑leaf path equals target sum.\n- **Validate BST**: Ensure inorder traversal is strictly increasing.\n- **Serialize/Deserialize**: Convert tree to string (e.g., level‑order) and back.\n- **Tree Construction**: Build from inorder + preorder/postorder sequences.\n\n## 7. Edge Cases & Best Practices\n\n- **Empty Tree**: Handle null root.\n- **Single Node**: Operations are trivial but ensure pointers handled.\n- **Duplicate Values**: Define policy (e.g., left‑equal or skip).\n- **Stack Depth**: For deep trees, consider iterative traversal to avoid recursion limits.\n\n## 8. Applications\n\n- **Databases & File Systems**: B‑Trees and variants for indexing.\n- **Expression Parsing**: Abstract Syntax Trees (ASTs).\n- **Autocomplete**: Tries (prefix trees).\n\n## 9. Resources & Further Reading\n\n- [GeeksforGeeks: Tree Data Structure](https://www.geeksforgeeks.org/binary-tree-data-structure/)  \n- [LeetCode Explore: Trees](https://leetcode.com/explore/learn/card/data-structure-tree/)  \n- CLRS §12: Elementary Trees\n",
    concepts: [
      "Binary trees and n-ary trees",
      "Tree traversal (inorder, preorder, postorder)",
      "Binary Search Trees (BST)",
      "AVL trees and balancing",
      "Tree height and depth",
      "Lowest Common Ancestor",
    ],
    keyPoints: [
      "BST property: left < root < right",
      "Balanced trees ensure O(log n) operations",
      "Recursion is natural for tree problems",
      "Consider iterative approaches to avoid stack overflow",
      "Tree traversal patterns are fundamental",
    ],
    timeComplexity: "Search/Insert/Delete: O(log n) balanced, O(n) worst case",
    spaceComplexity: "O(h) where h is height, O(log n) for balanced trees",
    commonPatterns: [
      "Tree traversal",
      "Path sum problems",
      "Tree construction",
      "Validate BST",
      "Serialize/Deserialize",
    ],
    practiceProblems: [94, 102, 124],
  },
  {
    id: "heaps-priority-queues",
    name: "Heaps & Priority Queues",
    difficulty: "Intermediate",
    timeEstimate: "2-3 weeks",
    domains: ["Heap"],
    description:
      "# Heaps & Priority Queues\n\nHeaps are **complete binary trees** represented in arrays that satisfy the **heap property**, making them ideal for efficient priority-based operations. A **priority queue** is an abstract data type where each element has a “priority” and the element with the highest (or lowest) priority is served before others.\n\n## 1. Heap Types\n\n- **Max‑Heap**  \n  Every parent node ≥ its children. Root is the maximum element.\n- **Min‑Heap**  \n  Every parent node ≤ its children. Root is the minimum element.\n\n## 2. Array Representation\n\n- Store the tree in a zero‑based array `A[]`.  \n  - Parent of `A[i]` is at `A[Math.floor((i - 1) / 2)]`.  \n  - Left child at `A[2*i + 1]`, right child at `A[2*i + 2]`.\n- Complete binary tree ensures no “holes” in the array.\n\n## 3. Core Operations\n\n1. **Heapify (sift‑down)**  \n   - Ensure a subtree rooted at index `i` satisfies heap property.  \n   - Compare with children, swap with the larger (max‑heap) or smaller (min‑heap) child, and recurse → _O(log n)_.\n\n2. **Build‑Heap**  \n   - Call `heapify` on all non‑leaf nodes from `⌊n/2⌋ - 1` down to `0` → _O(n)_ total.\n\n3. **Insert**  \n   - Append new element at end, “sift‑up” to restore property → _O(log n)_.\n\n4. **Extract‑Max/Min**  \n   - Swap root with last, remove last, then `heapify` root → _O(log n)_.\n\n5. **Peek**  \n   - Return `A[0]` without removal → _O(1)_.\n\n## 4. Advanced Algorithms\n\n- **Heap Sort**  \n  Build max‑heap, then repeatedly swap root with last element, reduce heap size by one, and `heapify` root → _O(n log n)_ total, in‑place.\n\n- **K‑Way Merge**  \n  Use a min‑heap of size `k` to merge `k` sorted lists/arrays in _O(n log k)_ time.\n\n- **Top K Elements**  \n  Maintain a min‑heap of size `k` while scanning the array → _O(n log k)_.\n\n## 5. Use Cases & Patterns\n\n- **Priority Queue** implementations (task scheduling, Dijkstra’s algorithm).  \n- **Median from Data Stream** using two heaps (min‑heap for upper half, max‑heap for lower half).  \n- **Meeting Rooms**: Schedule conflicts by sorting and scanning with a min‑heap of end times.\n\n## 6. Edge Cases & Tips\n\n- **Empty Heap**: Check size before `peek` or `extract`.  \n- **Duplicate Priorities**: Decide stable vs. unstable behavior as needed.  \n- **Dynamic Resizing**: Ensure array grows to accommodate inserts.\n\n## 7. Resources & Further Reading\n\n- [GeeksforGeeks: Heap Data Structure](https://www.geeksforgeeks.org/heap-data-structure/)  \n- [LeetCode Explore: Heaps and Maps](https://leetcode.com/explore/learn/card/heap-and-map/)  \n- CLRS §6.1–6.4: Heaps and Heap Sort\n",
    concepts: [
      "Min-heap and max-heap properties",
      "Heap implementation using arrays",
      "Heapify operations",
      "Priority queue operations",
      "Heap sort algorithm",
      "K-way merge problems",
    ],
    keyPoints: [
      "Parent-child relationship in array representation",
      "Heapify maintains heap property",
      "Useful for finding kth largest/smallest elements",
      "Efficient for repeated min/max operations",
      "Consider using built-in priority queue libraries",
    ],
    timeComplexity: "Insert/Delete: O(log n), Peek: O(1), Build heap: O(n)",
    spaceComplexity: "O(n) for storage",
    commonPatterns: [
      "Top K elements",
      "Merge K sorted arrays",
      "Meeting rooms",
      "Median from data stream",
      "Task scheduling",
    ],
    practiceProblems: [],
  },
  {
    id: "graphs",
    name: "Graph Algorithms",
    difficulty: "Advanced",
    timeEstimate: "4-5 weeks",
    domains: ["Graph"],
    description:
      "# Graph Algorithms\n\nGraphs are powerful, flexible data structures consisting of **vertices** (nodes) and **edges** (connections). They model complex relationships and networks—from social graphs to transportation systems—and require specialized algorithms for traversal, analysis, and optimization.\n\n## 1. Representations\n\n- **Adjacency List**  \n  Map each vertex to a list of its neighbors. Ideal for sparse graphs: _O(V + E)_ space.\n\n- **Adjacency Matrix**  \n  V×V matrix with `1` (or weight) where an edge exists. Good for dense graphs: _O(V²)_ space, constant‐time edge queries.\n\n- **Edge List**  \n  Simple array of edge pairs (u, v). Useful for algorithms like Kruskal’s MST.\n\n## 2. Core Traversals\n\n### Depth‑First Search (DFS)\n\n1. **Recursive** or **stack‑based** exploration.  \n2. Time: _O(V + E)_.  \n3. Use cases: cycle detection, topological sort, connectivity checks.\n\n### Breadth‑First Search (BFS)\n\n1. Queue‑based, level‑order traversal.  \n2. Time: _O(V + E)_.  \n3. Finds shortest path in unweighted graphs, connected components.\n\n## 3. Topological Sorting\n\n- Order vertices in a DAG so that for every directed edge u→v, u comes before v.  \n- Implement via DFS post‑order or Kahn’s algorithm using in‐degree queue.  \n- Time: _O(V + E)_.\n\n## 4. Shortest Path Algorithms\n\n- **Unweighted (BFS)**: _O(V + E)_.\n- **Dijkstra’s Algorithm**  \n  - Uses min‐heap, handles nonnegative weights.  \n  - Time: _O((V + E) log V)_.\n- **Bellman‑Ford**  \n  - Handles negative weights, detects negative cycles.  \n  - Time: _O(V·E)_.\n- **Floyd‑Warshall**  \n  - All‑pairs shortest paths.  \n  - Time: _O(V³)_.\n\n## 5. Minimum Spanning Trees (MST)\n\n- **Kruskal’s Algorithm**  \n  Sort edges, use Union‑Find to avoid cycles.  \n  Time: _O(E log E)_.\n\n- **Prim’s Algorithm**  \n  Grow tree from a starting vertex, using a min‑heap.  \n  Time: _O((V + E) log V)_.\n\n## 6. Advanced Patterns & Techniques\n\n- **Union‑Find (Disjoint Set)**  \n  Efficient connectivity queries and dynamic merges (_α(n)_ amortized).\n\n- **Cycle Detection**  \n  - Undirected: Union‑Find or DFS with parent check.  \n  - Directed: DFS color marking (white/gray/black).\n\n- **Bidirectional Search**  \n  Run BFS from both source and target to meet in the middle—halves search space.\n\n- **Heuristics & A***  \n  Incorporate domain‐specific cost estimates for faster pathfinding.\n\n## 7. Applications & Considerations\n\n- **Network Routing**, **Recommendation Systems**, **Dependency Resolution**, **Social Network Analysis**.\n- **Memory**: sparse vs dense representation trade‑offs.\n- **Edge Cases**: disconnected graphs, self‑loops, parallel edges, very large V or E.\n\n## 8. Resources & Further Reading\n\n- [GeeksforGeeks: Graph Data Structure](https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/)  \n- [LeetCode Explore: Graphs](https://leetcode.com/explore/learn/card/graph/)  \n- CLRS §22–24: Graph Algorithms (Introduction to Algorithms)\n",
    concepts: [
      "Graph representation (adjacency list/matrix)",
      "Depth-First Search (DFS)",
      "Breadth-First Search (BFS)",
      "Topological sorting",
      "Shortest path algorithms",
      "Minimum spanning trees",
    ],
    keyPoints: [
      "Choose appropriate representation for the problem",
      "DFS uses stack (recursive or explicit)",
      "BFS uses queue for level-by-level traversal",
      "Detect cycles using DFS",
      "Union-Find for connectivity problems",
    ],
    timeComplexity: "DFS/BFS: O(V + E), Dijkstra: O((V + E) log V)",
    spaceComplexity: "O(V + E) for adjacency list, O(V²) for matrix",
    commonPatterns: [
      "Graph traversal",
      "Cycle detection",
      "Connected components",
      "Shortest path",
      "Topological sort",
    ],
    practiceProblems: [200, 207, 329],
  },
  {
    id: "dynamic-programming",
    name: "Dynamic Programming",
    difficulty: "Advanced",
    timeEstimate: "4-6 weeks",
    domains: ["Dynamic Programming"],
    description:
      "# Dynamic Programming\n\nDynamic Programming (DP) is a powerful optimization technique used to solve complex problems by breaking them down into simpler subproblems, storing the results of those subproblems, and reusing them to avoid redundant work.\n\n## 1. Core Principles\n\n- **Overlapping Subproblems**  \n  Many subproblems recur multiple times.\n- **Optimal Substructure**  \n  An optimal solution to the problem contains optimal solutions to its subproblems.\n\n## 2. Standard Solution Workflow\n\n1. **Characterize the Structure of an Optimal Solution**  \n   Define what your state represents (e.g., `dp[i]`, `dp[i][j]`, `dp[mask]`).\n2. **Define the State & Transition**  \n   - State: parameters needed to describe a subproblem.  \n   - Transition: formula to compute `dp[state]` from smaller states.\n3. **Identify Base Cases**  \n   Initialize smallest subproblems explicitly (e.g., `dp[0]`, `dp[i][0]`).\n4. **Choose an Implementation**  \n   - **Memoization (Top‑Down)**: recursion + cache.  \n   - **Tabulation (Bottom‑Up)**: iterative table filling.\n5. **Space Optimization (Optional)**  \n   When possible, reduce dimensions (e.g., use rolling arrays for 2D → 1D).\n\n## 3. Common DP Patterns\n\n- **Linear DP (1D)**: Fibonacci, Climbing Stairs, House Robber.  \n- **Grid DP (2D)**: Unique Paths, Minimum Path Sum.  \n- **Interval DP**: Matrix Chain Multiplication, Burst Balloons.  \n- **Tree DP**: Tree Diameter, Maximum Path Sum in Binary Tree.  \n- **Bitmask DP**: Traveling Salesman, Subset DP on small sets.\n\n## 4. Pitfalls & Best Practices\n\n- Forgetting to handle base cases can lead to incorrect results or infinite recursion.\n- In tabulation, ensure you fill the table in the correct dependency order.\n- Watch out for high memory usage—consider whether you can compress dimensions.\n- Carefully choose state representation to avoid unnecessary dimensions.\n\n## 5. Example Problem Templates\n\n- **Knapsack**: `dp[i][w]` = max value using first `i` items with weight ≤ `w`.  \n- **Longest Increasing Subsequence**: `dp[i]` = length ending at index `i`, transition over all `j < i`.\n- **Edit Distance**: `dp[i][j]` = min operations to convert first `i` chars of `word1` to first `j` of `word2`.\n\n## 6. Resources & Further Reading\n\n- [GeeksforGeeks: Dynamic Programming](https://www.geeksforgeeks.org/dynamic-programming/)  \n- [LeetCode Explore: Dynamic Programming](https://leetcode.com/explore/learn/card/dynamic-programming/)  \n- CLRS §15: Dynamic Programming (Introduction to Algorithms)\n",
    concepts: [
      "Overlapping subproblems",
      "Optimal substructure",
      "Memoization (top-down)",
      "Tabulation (bottom-up)",
      "State transition",
      "Space optimization",
    ],
    keyPoints: [
      "Identify the recursive structure",
      "Define state and transition clearly",
      "Consider base cases carefully",
      "Optimize space when possible",
      "Practice pattern recognition",
    ],
    timeComplexity: "Typically O(n²) or O(n³) depending on states",
    spaceComplexity: "O(n) to O(n²) depending on implementation",
    commonPatterns: [
      "Linear DP (1D)",
      "Grid DP (2D)",
      "Interval DP",
      "Tree DP",
      "Bitmask DP",
    ],
    practiceProblems: [70, 198, 10],
  },
  {
    id: "backtracking",
    name: "Backtracking",
    difficulty: "Advanced",
    timeEstimate: "2-3 weeks",
    domains: ["Backtracking"],
    description:
      "# Backtracking\n\nBacktracking is a **depth‑first**, recursive algorithmic paradigm for solving constraint‑satisfaction and combinatorial search problems. It builds candidate solutions incrementally, abandoning (“backtracking”) a branch as soon as it determines that this branch cannot possibly lead to a valid complete solution.\n\n## 1. Core Principles\n\n- **Choice**: At each step, choose among a set of alternatives.\n- **Constraint**: Check whether the current partial solution violates any requirements.\n- **Goal**: Determine when a full valid solution has been constructed.\n\n## 2. General Pseudocode\n\n```pseudo\nfunction BACKTRACK(state):\n  if isSolution(state):\n    recordSolution(state)\n    return\n\n  for choice in generateChoices(state):\n    if isValid(choice, state):\n      apply(choice, state)\n      BACKTRACK(state)\n      undo(choice, state)\n```\n\n- **generateChoices**: Enumerate possible moves/extensions.\n- **isValid**: Prune early by checking constraints.\n- **apply/undo**: Modify and restore the state in-place to avoid extra copies.\n\n## 3. Pruning Strategies\n\n- **Constraint Checking**: Immediately discard any choice that violates rules (e.g., two queens attacking each other).\n- **Bounded Search**: Use problem‑specific bounds (e.g., sum limits) to cut branches early.\n- **Heuristic Ordering**: Try more promising choices first to find solutions quickly.\n\n## 4. Common Patterns & Examples\n\n- **Generate All Permutations**  \n  Recursively swap elements to produce every ordering of an array.\n\n- **N‑Queens**  \n  Place N queens on an N×N board so none attack each other; use row‑by‑row placement with column and diagonal checks.\n\n- **Sudoku Solver**  \n  Fill empty cells one by one, checking row, column, and 3×3‑box constraints.\n\n- **Subset / Combination Generation**  \n  Recurse by including or excluding each element to list all subsets.\n\n- **Word Search**  \n  DFS through grid cells, marking visited cells and backtracking on dead ends.\n\n## 5. Complexity & Optimization\n\n- **Time Complexity**: Typically exponential (e.g., O(2ⁿ), O(n!)) without pruning; effective pruning can drastically reduce the search tree.\n- **Space Complexity**: O(d) for recursion stack, where d is maximum depth of recursion (e.g., N in N‑Queens).\n\n## 6. Tips & Best Practices\n\n- **In‑Place State Updates**: Modify the existing data structures (arrays, boards) and undo changes to minimize memory overhead.\n- **Early Exit**: If you only need one solution, return immediately upon finding it.\n- **Memoization**: Cache subproblem results when overlapping substructures exist (turns into DP).\n- **Iterative Deepening**: Combine with depth‑limited search to find shortest solutions first.\n\n## 7. When to Use\n\n- Problems requiring enumeration of all valid configurations.\n- Constraint satisfaction (puzzles, assignment problems).\n- Search problems where greedy/DP approaches do not directly apply.\n\n## 8. Further Reading\n\n- [GeeksforGeeks: Backtracking Algorithms](https://www.geeksforgeeks.org/backtracking-algorithms/)\n- [LeetCode Explore: Backtracking](https://leetcode.com/explore/learn/card/backtracking/)\n- CLRS §4.2: Elementary Backtracking Techniques\n",
    concepts: [
      "Exhaustive search with pruning",
      "Recursive exploration",
      "State space tree",
      "Choice, constraint, and goal",
      "Pruning strategies",
      "Constraint satisfaction",
    ],
    keyPoints: [
      "Make a choice, explore, and backtrack",
      "Use pruning to improve efficiency",
      "Maintain and restore state carefully",
      "Consider iterative approaches for optimization",
      "Time complexity is often exponential",
    ],
    timeComplexity: "Often O(2ⁿ) or O(n!) in worst case",
    spaceComplexity: "O(depth) for recursion stack",
    commonPatterns: [
      "Generate all combinations/permutations",
      "N-Queens problem",
      "Sudoku solver",
      "Word search",
      "Subset generation",
    ],
    practiceProblems: [],
  },
  {
    id: "greedy-algorithms",
    name: "Greedy Algorithms",
    difficulty: "Intermediate",
    timeEstimate: "2-3 weeks",
    domains: ["Greedy"],
    description:
      "# Greedy Algorithms\n\nGreedy algorithms build a solution by making a sequence of choices, each of which looks best at the moment (the **greedy choice property**). If those local decisions also lead to a global optimum, the algorithm is correct. Greedy methods are typically fast and easy to implement but require a proof of correctness for each problem.\n\n## 1. Key Principles\n\n1. **Greedy Choice Property**  \n   - A globally optimal solution can be arrived at by making locally optimal (greedy) choices.\n2. **Optimal Substructure**  \n   - An optimal solution to the problem contains optimal solutions to its subproblems.\n\n## 2. Common Greedy Paradigms\n\n- **Activity Selection / Interval Scheduling**  \n  Choose the next activity that finishes earliest to maximize the number of non-overlapping activities.\n\n- **Fractional Knapsack**  \n  Take items in decreasing order of value-to-weight ratio; you can take fractional parts of items.\n\n- **Huffman Coding**  \n  Build an optimal prefix code by repeatedly merging the two least‐frequent symbols.\n\n- **Minimum Spanning Trees**  \n  - **Kruskal’s Algorithm**: Sort edges by weight and add if they don’t form a cycle (use Union‑Find).  \n  - **Prim’s Algorithm**: Grow a tree by repeatedly adding the cheapest edge from the tree to a new vertex.\n\n- **Job / Task Scheduling**  \n  Schedule tasks to minimize lateness or maximize throughput by ordering by deadlines or durations.\n\n- **Coin Change (Canonical Systems)**  \n  Take the largest coin repeatedly when the coin system is canonical (e.g., U.S. coins).\n\n- **Gas Station Problem**  \n  Find a start point such that you can complete the circuit by always driving as far as possible before refueling.\n\n## 3. Implementation Steps\n\n1. **Sort** the input (e.g., by weight, finish time, ratio) if required — _O(n log n)_.  \n2. **Iterate** through sorted items/intervals/tasks.  \n3. **Select** or **discard** based on feasibility and greedy criteria — _O(n)_.\n4. **Construct** the final solution (e.g., list of chosen items or schedule).\n\n## 4. Proving Correctness\n\n- **Exchange Argument**: Show that any optimal solution can be transformed into the greedy solution by exchanging choices.\n- **Mathematical Induction**: Prove that if greedy choice holds for smaller instances, it holds for larger ones.\n\n## 5. Time & Space Complexity\n\n- **Time**: Dominated by sorting → _O(n log n)_ in most cases.  \n- **Space**: Typically _O(1)_ additional space (in-place), or _O(n)_ if building auxiliary data structures.\n\n## 6. When to Use Greedy\n\n- Problems with **canonical** structures (e.g., fractional knapsack, Huffman coding).  \n- Scheduling and covering problems where local decisions lead to a global optimum.  \n- When you can prove the greedy choice property and optimal substructure.\n\n## 7. Edge Cases & Pitfalls\n\n- Greedy fails when local optima don’t lead to global optimum (e.g., 0/1 knapsack, some coin systems).  \n- Always verify with counterexamples before trusting a greedy approach.\n\n## 8. Additional Resources\n\n- [Greedy Algorithms on GeeksforGeeks](https://www.geeksforgeeks.org/greedy-algorithms/)  \n- [LeetCode Explore: Greedy](https://leetcode.com/explore/learn/card/greedy/)  \n- CLRS §16: Greedy Algorithms\n",
    concepts: [
      "Greedy choice property",
      "Optimal substructure",
      "Activity selection",
      "Huffman coding",
      "Minimum spanning trees",
      "Interval scheduling",
    ],
    keyPoints: [
      "Verify greedy choice leads to optimal solution",
      "Greedy doesn't always work",
      "Often simpler than dynamic programming",
      "Sorting is frequently involved",
      "Prove correctness carefully",
    ],
    timeComplexity: "Often O(n log n) due to sorting",
    spaceComplexity: "Usually O(1) additional space",
    commonPatterns: [
      "Interval problems",
      "Fractional knapsack",
      "Job scheduling",
      "Minimum coins",
      "Gas station",
    ],
    practiceProblems: [],
  },
  {
    id: "binary-search",
    name: "Binary Search",
    difficulty: "Intermediate",
    timeEstimate: "1-2 weeks",
    domains: ["Binary Search"],
    description:
      "# Binary Search\n\nBinary Search is a classic divide‑and‑conquer algorithm that efficiently locates a target value within a **sorted** (or otherwise monotonic) search space by repeatedly halving the interval.\n\n## 1. Principle\n\n1. Start with two pointers, `left = 0` and `right = n - 1`.\n2. Compute `mid = left + (right - left) // 2` to avoid overflow.\n3. Compare `A[mid]` with the target:\n   - If equal, return `mid`.\n   - If `A[mid] < target`, search the right half: `left = mid + 1`.\n   - If `A[mid] > target`, search the left half: `right = mid - 1`.\n4. Repeat until `left > right` (target not found).\n\n## 2. Iterative Implementation\n\n```python\nleft, right = 0, n - 1\nwhile left <= right:\n    mid = left + (right - left) // 2\n    if A[mid] == target:\n        return mid\n    elif A[mid] < target:\n        left = mid + 1\n    else:\n        right = mid - 1\nreturn -1\n```\n\n## 3. Recursive Implementation\n\n```python\ndef binary_search(A, left, right, target):\n    if left > right:\n        return -1\n    mid = left + (right - left) // 2\n    if A[mid] == target:\n        return mid\n    elif A[mid] < target:\n        return binary_search(A, mid + 1, right, target)\n    else:\n        return binary_search(A, left, mid - 1, target)\n```\n\n## 4. Variations & Advanced Uses\n\n- **First/Last Occurrence**: Adjust boundaries to find the leftmost or rightmost index matching the target.\n- **Search in Rotated Array**: Determine which half is sorted, then binary‑search in the correct half.\n- **Find Peak Element**: Compare mid with neighbors to decide search direction in an “unimodal” array.\n- **Binary Search on Answer**: Use when decision function is monotonic (e.g., minimize maximum load, capacity problems).\n- **2D Matrix Search**: Treat matrix as a flattened sorted array or apply two‑phase binary search (row then column).\n\n## 5. Template‑Based Approach\n\nKeep a reusable template with clearly defined updates to `left` and `right`, and consistent mid calculation:\n\n```text\ninitialize left, right\ndo while left <= right:\n    compute mid = left + (right - left) // 2\n    if match: return\n    update left or right based on comparison\nreturn not found\n```\n\n## 6. Boundary & Edge Cases\n\n- **Empty array**: immediately return not found.\n- **Single element**: ensure `left <= right` condition covers it.\n- **Overflow**: always use `left + (right - left) // 2` instead of `(left + right) // 2` in languages with fixed‑width ints.\n- **Infinite loop**: ensure `left` and `right` update strictly (e.g., `mid + 1` or `mid - 1`).\n\n## 7. Resources & Further Reading\n\n- [GeeksforGeeks: Binary Search](https://www.geeksforgeeks.org/binary-search/)\n- [LeetCode Explore: Binary Search](https://leetcode.com/explore/learn/card/binary-search/)\n- CLRS §2.3: Binary Search (Introduction to Algorithms)\n",
    concepts: [
      "Search space reduction",
      "Left and right boundaries",
      "Search condition and updates",
      "Binary search on answer",
      "Peak finding",
      "Rotated arrays",
    ],
    keyPoints: [
      "Array must be sorted (or search space monotonic)",
      "Careful with boundary conditions",
      "Avoid infinite loops",
      "Consider using binary search on non-array problems",
      "Template-based approach helps consistency",
    ],
    timeComplexity: "O(log n)",
    spaceComplexity: "O(1) iterative, O(log n) recursive",
    commonPatterns: [
      "Find exact target",
      "Find boundary (first/last occurrence)",
      "Search in rotated array",
      "Find peak element",
      "Search in 2D matrix",
    ],
    practiceProblems: [704, 4],
  },
  {
    id: "bit-manipulation",
    name: "Bit Manipulation",
    difficulty: "Intermediate",
    timeEstimate: "1-2 weeks",
    domains: ["Bit Manipulation"],
    description:
      "# Bit Manipulation\n\nBit manipulation refers to the process of directly performing operations on the binary representation of integers. It is highly efficient, often offering **O(1)** or **O(log n)** solutions, and is frequently used in low-level programming, optimization problems, and algorithmic challenges.\n\n## 1. Common Bitwise Operators\n\n| Operator | Symbol | Description               | Example (a = 5, b = 3) |\n|----------|--------|---------------------------|------------------------|\n| AND      | `&`    | Sets each bit to 1 if both are 1 | `5 & 3 = 1`           |\n| OR       | `|`    | Sets each bit to 1 if either is 1 | `5 | 3 = 7`         |\n| XOR      | `^`    | Sets each bit to 1 if only one is 1 | `5 ^ 3 = 6`     |\n| NOT      | `~`    | Inverts all bits            | `~5 = -6` (in 2's comp) |\n| LEFT SHIFT | `<<` | Shifts bits to the left (multiplies by 2) | `5 << 1 = 10` |\n| RIGHT SHIFT | `>>` | Shifts bits to the right (divides by 2) | `5 >> 1 = 2`   |\n\n## 2. Useful Bit Tricks\n\n### a. **Check if number is even/odd**\n```js\nn % 2 == 0 ➝ (n & 1) == 0\n```\n\n### b. **Check if a number is power of 2**\n```js\n(n > 0) && (n & (n - 1)) == 0\n```\n\n### c. **Count set bits (Brian Kernighan's Algo)**\n```js\nwhile (n) {\n  n = n & (n - 1);\n  count++;\n}\n```\n\n### d. **XOR to find unique number in array**\n```js\n// If every element appears twice except one:\nres = 0;\nfor (num of array) res ^= num;\n```\n\n### e. **Set / Clear / Toggle ith bit**\n```js\nn | (1 << i)     // Set ith bit\nn & ~(1 << i)    // Clear ith bit\nn ^ (1 << i)     // Toggle ith bit\n```\n\n## 3. Applications\n\n- **Optimization**: Perform constant-time operations without arithmetic.\n- **Subset Generation**: Represent presence/absence using bits.\n- **Bitmasking**: Track state (e.g. visited nodes) in small memory.\n- **Cryptography and Hashing**: Heavily rely on bitwise operations.\n\n## 4. Two’s Complement (Negative Numbers)\n\nIn most systems, negative integers are stored using two’s complement:\n```js\n-5 = ~5 + 1 → (~00000101) + 1 = 11111011\n```\nUnderstanding this is crucial for interpreting bitwise results with signed integers.\n\n## 5. Tips for Mastery\n\n- Learn binary representation of numbers.\n- Practice shifting and masking bits.\n- Write code to simulate operations manually.\n- Visualize bits when debugging.\n\n## 6. Resources & Further Reading\n\n- [Bit Manipulation Cheatsheet (LeetCode)](https://leetcode.com/explore/featured/card/bit-manipulation/)\n- [GeeksforGeeks Bit Manipulation](https://www.geeksforgeeks.org/bitwise-operators-in-c-cpp/)\n- Cracking the Coding Interview: Bit Manipulation Chapter\n",
    concepts: [
      "Bitwise operators (AND, OR, XOR, NOT)",
      "Bit shifting (left and right)",
      "Bit masking",
      "Two's complement",
      "Counting set bits",
      "Power of 2 operations",
    ],
    keyPoints: [
      "XOR properties are very useful",
      "Bit manipulation is often O(1) or O(log n)",
      "Useful for space optimization",
      "Practice binary number representation",
      "Common in interview questions",
    ],
    timeComplexity: "Usually O(1) or O(log n)",
    spaceComplexity: "O(1)",
    commonPatterns: [
      "Single number problems",
      "Subset generation using bits",
      "Power of 2 detection",
      "Bit counting",
      "XOR properties",
    ],
    practiceProblems: [],
  },
  {
    id: "math-geometry",
    name: "Math & Geometry",
    difficulty: "Intermediate",
    timeEstimate: "2-3 weeks",
    domains: ["Math", "Geometry"],
    description:
      "# Math & Geometry\n\nMathematical and geometric problems are common in algorithmic challenges and require a strong foundation in number theory, algebra, and spatial reasoning. These problems often involve applying formulas, identifying patterns, and making logical deductions.\n\n## 1. Number Theory Basics\n\n- **Prime Numbers**: Use the Sieve of Eratosthenes to precompute primes up to _n_.\n- **Factorization**: Efficient factorization techniques help in GCD/LCM problems.\n- **GCD & LCM**:\n  - `GCD(a, b)` can be computed using Euclid’s algorithm: `gcd(a, b) = gcd(b, a % b)`.\n  - `LCM(a, b) = (a * b) / GCD(a, b)`.\n- **Divisibility & Modulo Rules**: Understand when to use modulo to avoid overflow or compare results cyclically.\n\n## 2. Modular Arithmetic\n\n- **Modular Addition/Multiplication**:\n  - `(a + b) % m = ((a % m) + (b % m)) % m`\n  - `(a * b) % m = ((a % m) * (b % m)) % m`\n- **Modular Exponentiation**:\n  - Used in large power computations (e.g., RSA encryption). Implement using fast exponentiation: _O(log n)_.\n- **Modular Inverse**:\n  - Use Fermat’s Little Theorem when modulus is prime: `a⁻¹ ≡ a^(m-2) mod m`.\n\n## 3. Geometry & Coordinate Problems\n\n- **Distance**: Between points `(x1, y1)` and `(x2, y2)`:\n  - Euclidean: `√((x2 - x1)² + (y2 - y1)²)`\n  - Manhattan: `|x2 - x1| + |y2 - y1|`\n\n- **Basic Shapes**:\n  - Area of triangle, circle, rectangle\n  - Pythagorean theorem for right triangles\n\n- **Orientation & Collinearity**:\n  - Cross product to determine orientation: `A × B > 0 → counter-clockwise`\n  - Three points are collinear if the area formed is zero.\n\n## 4. Combinatorics & Counting\n\n- **Factorials & Binomial Coefficients**:\n  - `n!`, `nCr`, Pascal’s Triangle, combinations and permutations\n- **Pigeonhole Principle** and **Inclusion-Exclusion** for problem solving\n- **Catalan numbers**, **derangements**, **Fibonacci**, etc., for specific types of problems\n\n## 5. Patterns & Optimization Techniques\n\n- Many math problems rely on **recognizing patterns** and applying known formulas.\n- Optimize loops with precomputation, math tricks (e.g., arithmetic series sum, geometric series, floor/ceil properties).\n- Avoid floating-point precision issues where possible. Use integer math or proper rounding.\n\n## 6. Edge Case Handling\n\n- **Integer Overflow**: Use long types or modulo operations\n- **Precision**: Avoid floating-point comparisons directly (use `abs(a - b) < epsilon`)\n- **Negative Modulo**: Standardize results using `(a % m + m) % m`\n\n## 7. Resources & Further Reading\n\n- [Art of Problem Solving: Number Theory](https://artofproblemsolving.com/wiki/index.php/Number_theory)\n- [GeeksforGeeks Math Section](https://www.geeksforgeeks.org/mathematics-gq/)\n- [CP Algorithms – Math](https://cp-algorithms.com/)\n- Khan Academy: Geometry, Algebra, and Modular Arithmetic\n",
    concepts: [
      "Number theory basics",
      "Prime numbers and factorization",
      "GCD and LCM",
      "Modular arithmetic",
      "Coordinate geometry",
      "Combinatorics",
    ],
    keyPoints: [
      "Understand mathematical properties",
      "Look for patterns and formulas",
      "Handle edge cases (overflow, precision)",
      "Use mathematical insights to optimize",
      "Practice mental math and estimation",
    ],
    timeComplexity: "Varies by problem",
    spaceComplexity: "Usually O(1)",
    commonPatterns: [
      "Mathematical formulas",
      "Pattern recognition",
      "Prime number operations",
      "Coordinate calculations",
      "Probability and statistics",
    ],
    practiceProblems: [],
  },
];
